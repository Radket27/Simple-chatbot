{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ip7j4nN_y-5t"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes, accelerate\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "device = \"cuda\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if (tokenizer.pad_token is None):\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             device_map=\"auto\",\n",
        "                                             load_in_8bit=True)"
      ],
      "metadata": {
        "id": "-NOZkXaxzDis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(save):\n",
        "    save = save.split()\n",
        "    i = len(save) - 1\n",
        "    a1 = True\n",
        "    while(a1):\n",
        "        if(save[i] == '[/INST]'):\n",
        "            for x in range(i):\n",
        "                save.pop(0)\n",
        "            save.pop(0)\n",
        "            a1 = False\n",
        "        else:\n",
        "            i += -1\n",
        "    save = ' '.join(save)\n",
        "    return save\n",
        "text = []\n",
        "a1 = True\n",
        "print(\"To exit type (Y)\")\n",
        "while a1:\n",
        "    message = input(\"User: \")\n",
        "    if(message == 'Y'):\n",
        "        a1 = False\n",
        "    else:\n",
        "        text.append({\"role\":\"user\",\"content\":message})\n",
        "        inputs = tokenizer.apply_chat_template(text,return_tensors=\"pt\",\n",
        "                                               tokenize=True,\n",
        "                                               add_generation_prompt=True).to(device)\n",
        "        generation_kwargs = {\"repetition_penalty\": 1.3, \"max_new_tokens\": 50}\n",
        "        outputs = model.generate(inputs,**generation_kwargs,\n",
        "                                 pad_token_id=tokenizer.pad_token_id)\n",
        "        save = tokenizer.decode(outputs[0])\n",
        "        save1 = chat(save)\n",
        "        text.append({\"role\":\"assistant\",\"content\":f\" {save1}\"})\n",
        "        print(f\"Assistant: {save1}\")"
      ],
      "metadata": {
        "id": "3_rZzofBzF3P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}